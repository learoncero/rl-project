{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook description\n",
    "\n",
    "This notebook demonstrates the customization of the reward function for the `highway-fast-v0 environment`. The custom reward logic is implemented in the `utils/highwayEnvCustomReward.py` file and replaces or enhances elements of the default reward function to better align with realistic driving scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the default reward function in `highway-fast-v0`\n",
    "\n",
    "The default reward function in the highway-fast-v0 environment is designed to promote safe and efficient driving behaviors. It calculates a scalar reward for each action based on four key factors:\n",
    "\n",
    "1. **Collision Penalty:**\n",
    "\n",
    "   - The `collision_reward` penalizes the agent if the vehicle crashes. This is a binary penalty, either 0 (no crash) or a negative value (collision occurred).\n",
    "\n",
    "2. **Right Lane Reward:**\n",
    "\n",
    "   - The `right_lane_reward` rewards the agent for staying in the rightmost lane. The reward increases as the vehicle moves closer to the rightmost lane, encouraging proper lane usage.\n",
    "\n",
    "3. **High-Speed Reward:**\n",
    "\n",
    "   - The `high_speed_reward` incentivizes driving at higher speeds. The reward is proportional to the vehicle's forward speed, normalized within a defined speed range.\n",
    "\n",
    "4. **On-Road Reward:**\n",
    "   - The `on_road_reward` encourages the vehicle to stay on the road by multiplying the total reward by 1 if the vehicle is on the road or 0 if it is off-road. This means that if the vehicle is off-road, the total reward is also zero.\n",
    "\n",
    "## Shortcomings of the default reward function in `highway-fast-v0`\n",
    "\n",
    "1. **Overemphasising the rightmost lane:**\n",
    "\n",
    "   - The `right_lane_reward` encourages staying in the rightmost lane, which may not be in line with realistic driving goals. For example, overtaking may require swerving into other lanes, which is not directly encouraged.\n",
    "\n",
    "2. **Speed reward out of context:**\n",
    "\n",
    "   - The `high_speed_reward` rewards speed linearly, but does not take into account traffic density. Speeding in traffic jams should be penalised.\n",
    "\n",
    "3. **Binary Collision Penalty:**\n",
    "\n",
    "   - The `collision_reward` is binary, providing the same penalty regardless of the severity or cause of the crash. This ignores scenarios like near-misses, which could be penalized slightly to encourage caution.\n",
    "\n",
    "4. **Lack of a Safe Distance Mechanism:**\n",
    "\n",
    "   - The current reward function does not provide an incentive to maintain a safe distance from the vehicle in front. However, promoting a safe following distance is a fundamental part of safe driving. It helps avoid collisions and creates smoother traffic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes made to the reward function\n",
    "\n",
    "### Add safe distance reward\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "1. Identify the Front Vehicle:\n",
    "\n",
    "   - Use the existing `road.neighbour_vehicles(vehicle)` method to locate the vehicle ahead of the agent.\n",
    "\n",
    "2. Compute the Distance:\n",
    "\n",
    "   - Calculate the distance between the agent's vehicle and the identified front vehicle. If no front vehicle exists, assume the distance is infinite.\n",
    "\n",
    "3. Define the Reward:\n",
    "\n",
    "   - Reward the agent if the distance exceeds a safe threshold (e.g., 10 meters). Penalize the agent as the distance decreases below this threshold.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Encourages the agent to maintain a safe distance, reducing the likelihood of rear-end collisions.\n",
    "- Promotes safer and more realistic driving behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe distance reward\n",
    "front_vehicle, _ = self.road.neighbour_vehicles(self.vehicle) # identify the front vehicle\n",
    "safe_distance = 5\n",
    "if front_vehicle:\n",
    "    distance = max(front_vehicle.position[0] - self.vehicle.position[0], 0)\n",
    "\n",
    "    if distance > safe_distance:\n",
    "        safe_distance_reward = 1  # Full reward if distance is safe\n",
    "    else:\n",
    "        safe_distance_reward = -1 * (safe_distance - distance) / safe_distance\n",
    "else:\n",
    "    safe_distance_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve existing collision reward\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "1. Collision Penalty:\n",
    "    - Check whether the agent's vehicle has crashed (self.vehicle.crashed).\n",
    "    - Apply a fixed penalty of -1 if a collision has occurred; otherwise, no penalty is applied.\n",
    "\n",
    "2. Near-Miss Penalty:\n",
    "    - Iterate through all vehicles on the road and calculate the Euclidean distance between the agent's vehicle and other vehicles.\n",
    "    - Define a \"near-miss\" threshold (e.g., 2 meters).\n",
    "    - Apply a scaled penalty when the distance falls within the near-miss range (closer near-misses incur higher penalties).\n",
    "\n",
    "3. Combine Penalties:\n",
    "    - The total collision reward is the sum of the collision penalty and the near-miss penalties.\n",
    "\n",
    "**Advantages**\n",
    "- Encourages the agent to avoid collisions entirely by imposing a strict penalty.\n",
    "- Promotes safer behavior by penalizing close interactions (near-misses), even when collisions are avoided.\n",
    "- Improves realism in the simulation, as near-misses are indicative of risky driving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved collision reward\n",
    "collision_penalty = -1 if self.vehicle.crashed else 0  # Full penalty for collision\n",
    "near_miss_penalty = 0\n",
    "for vehicle in self.road.vehicles:\n",
    "    if vehicle is not self.vehicle:\n",
    "        distance_to_vehicle = np.linalg.norm(\n",
    "            np.array(self.vehicle.position) - np.array(vehicle.position)\n",
    "        )\n",
    "        if 0 < distance_to_vehicle <= 2:  # Near-miss threshold\n",
    "            near_miss_penalty += -0.5 * (2 - distance_to_vehicle) / 2\n",
    "\n",
    "collision_reward = collision_penalty + near_miss_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add high speed reward\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "1. Define Traffic Radius:\n",
    "    - Set a traffic_radius (e.g., 10 meters) within which the agent's vehicle assesses the surrounding traffic density.\n",
    "\n",
    "2. Count Nearby Vehicles:\n",
    "    - Iterate through all vehicles on the road and calculate the Euclidean distance between the agent's vehicle and others.\n",
    "    - Increment the count of nearby vehicles if they are within the defined traffic radius.\n",
    "\n",
    "3. Compute Traffic Density Factor:\n",
    "    - Use the number of nearby vehicles to calculate a traffic density factor, where higher traffic density reduces the reward.\n",
    "    - Define a maximum density (max_density, e.g., 10 vehicles) for scaling. The factor decreases linearly as the density approaches this maximum.\n",
    "\n",
    "4. Scale the High-Speed Reward:\n",
    "    - Adjust the agent's speed-based reward (scaled_speed) using the traffic density factor.\n",
    "    - Clip the reward within a range of 0 to 1 for consistency.\n",
    "\n",
    "**Advantages**\n",
    "- Encourages the agent to drive at higher speeds when traffic density is low, promoting efficient driving.\n",
    "- Discourages risky high-speed driving in dense traffic, reducing the likelihood of collisions or unsafe maneuvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high speed reward\n",
    "traffic_radius = 10\n",
    "\n",
    "# Count the number of vehicles within the traffic radius\n",
    "nearby_vehicles = 0\n",
    "for other_vehicle in self.road.vehicles:\n",
    "    if other_vehicle is not self.vehicle:\n",
    "        distance = np.linalg.norm(\n",
    "            np.array(other_vehicle.position) - np.array(self.vehicle.position)\n",
    "        )\n",
    "        if distance < traffic_radius:\n",
    "            nearby_vehicles += 1\n",
    "\n",
    "# Traffic density factor: more vehicles -> higher penalty\n",
    "max_density = 10\n",
    "traffic_density_factor = max(0, 1 - nearby_vehicles / max_density)\n",
    "\n",
    "# Adjust high-speed reward based on traffic density\n",
    "high_speed_reward = np.clip(scaled_speed * traffic_density_factor, 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
